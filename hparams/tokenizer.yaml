#############################################################################
###### Hyperparameters to train a tokenizer model using SentencePiece  ######
#############################################################################

####################### Data files ####################################
output_folder: ../models/V3_Final/tokenizer
train_log: !ref <output_folder>/train_log.txt

# Data files
data_folder: ../data/V3_Final/jsonl
manifest_folder: ../data/V3_Final/manifests
train_csv: !ref <manifest_folder>/train.csv
dev_csv: !ref <manifest_folder>/dev.csv
skip_prep: False

####################### Training Parameters ####################################
token_type: unigram  # ["unigram", "bpe", "char"]
token_output: 170  # index(blank/eos/bos/unk) = 0
character_coverage: 1.0
num_sequences: null # If null, use all sequences in the training data
csv_read: semantics

####################### Training Model ####################################
tokenizer: !name:speechbrain.tokenizers.SentencePiece.SentencePiece
   model_dir: !ref <output_folder>
   vocab_size: !ref <token_output>
   annotation_train: !ref <train_csv>
   annotation_read: !ref <csv_read>
   model_type: !ref <token_type> # ["unigram", "bpe", "char"]
   character_coverage: !ref <character_coverage>
   num_sequences: !ref <num_sequences>
   annotation_list_to_check: [!ref <train_csv>, !ref <dev_csv>]